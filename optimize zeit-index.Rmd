---
title: "Zeit classification of neutral evaluations"
author: "Dirk Ulbricht"
fontsize: 11pt
fig_caption: yes
output:
pdf_document:
keep_tex: yes

word_document: default
---

```{r,echo=FALSE,warning=FALSE,message=FALSE}
DirCode='C:/Users/Dirk/Documents/GitHub/zeit-2'
DirCode='h://Git/zeit-2'


library('irr')
df=read.csv(paste(DirCode,'/data/goldstandard/Annotatoren.csv',sep=''))
df=df[complete.cases(df),]
# getting automated sentiment
# load valueword (a vector of SentiWS words in the first column, values in the second, 
# ambigous duplicates are eliminated, capitalizations are preserved)
valueword=read.csv(paste(DirCode,'/valueword.csv',sep=''))



# creating function to evaluate texts and applying it:
sentiment<-function (text, valueword){
        # returns the word, value, stem, 
        # form and frequency of each sentiment word in text
        # in the data.frame valdf. And, it returns the total 
        # number of words in text
        # as integer nword.
        if (length(text) == 2 & text[1] == ",x") {
                text = text[2]
                }
        text.split = sapply(strsplit(text, " "), function(x) x)
        ind = valueword[, 1] %in% text.split
        valdf = valueword[ind, , drop = F]
        valdf$h = sapply(valueword[ind, 1], function(x) sum(text.split%in%x))
        nwords = length(text.split)
        return(list(valdf,nwords))
        }


# function to process the data
sentproc<-function(x){
        # processes the information returned by stentiment.R and returns 
        # a vector comprising number of positive words (npword)
        # number of negative words (nnword), total number of words (nword)
        # sum of positive values (pvalue), and sum of negative values (nvalue)
        valdf=x[[1]]
        pos.ind=valdf$wert>0
        neg.ind=valdf$wert<0
        result=c(
                npword=sum(pos.ind)
                ,nnword=sum(neg.ind)
                ,nword=x[[2]]
                ,pvalue=sum(valdf$wert[pos.ind])
                ,nvalue=sum(valdf$wert[neg.ind])
                ,value=sum(valdf$wert)
                )
}


```


```{r aggregating to articles-recoding-unambigous,echo=F}
# aggregating to articles
auto.sent=lapply(as.character(df[,'Text']),sentiment,valueword)
df=cbind(t(sapply(auto.sent,function(x) sentproc(x))),df)

# aggregate to article level
df.art=data.frame(aggregate(df$nword,list(df$Artikeln_Nr),sum))
colnames(df.art)=c('Article','nword')
df.art$JM=aggregate(df$nword*df$JM,list(df$Artikeln_Nr)
                    ,sum)[,2]/df.art$nword
df.art$SN=aggregate(df$nword*df$SN,list(df$Artikeln_Nr)
                    ,sum)[,2]/df.art$nword
df.art$DC=aggregate(df$nword*df$DC,list(df$Artikeln_Nr)
                    ,sum)[,2]/df.art$nword
df.art$value=aggregate(df$value,list(df$Artikeln_Nr)
                       ,mean)[,2]

mean.ann=mean(sapply(df.art[,c('SN','DC','JM')],function(x) x))
sd.ann=sd(sapply(df.art[,c('SN','DC','JM')],function(x) x))

# rescaling value to optimize visual comparison
df.art$value.rs=(df.art$value-mean(df.art$value))/
        sd(df.art$value)*sd.ann+mean.ann


df.art$ann=rowSums(df.art[,c('SN','DC','JM')])/3


plot(df.art$value.rs,type='line',col='blue')
lines(df.art$ann)
cor(df.art$ann,df.art$value.rs)

# df.art=df.art[df.art$value<-0.05|df.art$value>0.05,]

steps=df.art$nword[order(df.art$nword)]
corl=matrix(NA,nrow=length(steps),3)
for (i in 1:(length(steps)-2)){
        long.art=df.art$nword>=steps[i]
        corl[i]=cor(df.art$ann[long.art],df.art$value.rs[long.art])
        corl[i,2]=cor.test(df.art$ann[long.art],df.art$value.rs[long.art])$p.value
        corl[i,3]=sum(long.art)
}
```

```{r correlation and # of words,cache=FALSE}
plot(corl[,1],xaxt='n'
     ,main='Correlation and # of words'
     ,xlab='Number of words'
,ylab='Correlation')
xind=1:(length(steps)-1)
ind=corl[,2]<0.05
points(xind[which(ind)],corl[which(ind),1],col='red')
axis(1, at=1:length(steps), labels=steps)
legend('topleft'
       ,pch=1
       ,col=c('black','red')
       ,legend=c('correlation coefficients','significance at 5% level')
        )
# corl1=corl

```

```{r plotting significant and high correlation,eval=FALSE}
# plotting the results
long.art=df.art$nword>=steps[30]

plot(df.art$value[long.art],df.art$ann[long.art])
lines(df.art$JM[long.art])
lines(df.art$DC[long.art],col='blue')
lines(df.art$SN[long.art],col='red')
abline(h=0)
abline(h=-1,lty=2)
abline(h=1,lty=2)



```

```{r are there any nearly neutral words that can be dropped?}
# ignoring small positive and negative value-words


testrange=valueword[]
t(combn(x, 2))
border=data.frame(lower=c(0,0,-0.0048,-0.0048)
                  ,upper=c(0,0.004,0,0.004))
I=nrow(border)
corI=data.frame(matrix(NA,I,1))

for (i in 1:I){
        
valueword=read.csv(paste(DirCode,'/valueword.csv',sep=''))
big.ind=valueword$wert<border$lower[i]|border$upper[i]<valueword$wert
valueword=valueword[big.ind,]

auto.sent=lapply(as.character(df[,'Text']),sentiment,valueword)
auto=t(sapply(auto.sent,function(x) sentproc(x)))
df.art[,paste(border$lower[i],border$upper[i],sep=':')]=aggregate(auto[,'value'],list(df$Artikeln_Nr)
                       ,mean)[,2]
corI[i,1]=cor(df.art$ann,df.art[,paste(border$lower[i],border$upper[i],sep=':')])
}

```




---
title: "Zeit classification of neutral evaluations"
author: "Dirk Ulbricht"
fontsize: 11pt
fig_caption: yes
output:
pdf_document:
keep_tex: yes

word_document: default
---

```{r,echo=FALSE,warning=FALSE,message=FALSE}
DirCode='C:/Users/Dirk/Documents/GitHub/zeit-2'
# DirCode='h://Git/zeit-2'


library('irr')
df=read.csv(paste(DirCode,'/data/goldstandard/Annotatoren.csv',sep=''))
df=df[complete.cases(df),]
# getting automated sentiment
# load valueword (a vector of SentiWS words in the first column, values in the second, 
# ambigous duplicates are eliminated, capitalizations are preserved)
valueword=read.csv(paste(DirCode,'/valueword.csv',sep=''))



# creating function to evaluate texts and applying it:
sentiment<-function (text, valueword){
        # returns the word, value, stem, 
        # form and frequency of each sentiment word in text
        # in the data.frame valdf. And, it returns the total 
        # number of words in text
        # as integer nword.
        if (length(text) == 2 & text[1] == ",x") {
                text = text[2]
                }
        text=gsub("[[:punct:]]", "", text)
        text.split = sapply(strsplit(text, " "), function(x) x)
        ind = valueword[, 1] %in% text.split
        valdf = valueword[ind, , drop = F]
        valdf$h = sapply(valueword[ind, 1], function(x) sum(text.split%in%x))
        nwords = length(text.split)
        return(list(valdf,nwords))
        }


# function to process the data
sentproc<-function(x){
        # processes the information returned by stentiment.R and returns 
        # a vector comprising number of positive words (npword)
        # number of negative words (nnword), total number of words (nword)
        # sum of positive values (pvalue), and sum of negative values (nvalue)
        valdf=x[[1]]
        pos.ind=valdf$wert>0
        neg.ind=valdf$wert<0
        result=c(
                npword=sum(pos.ind)
                ,nnword=sum(neg.ind)
                ,nword=x[[2]]
                ,pvalue=sum(valdf$wert[pos.ind])
                ,nvalue=sum(valdf$wert[neg.ind])
                ,value=sum(valdf$wert)
                )
        }


```


```{r aggregating to articles recoding unambigous,echo=F,eval=TRUE}
# aggregating to articles
auto.sent=lapply(as.character(df[,'Text']),sentiment,valueword)
df=cbind(t(sapply(auto.sent,function(x) sentproc(x))),df)

# aggregate to article level
df.art=data.frame(aggregate(df$nword,list(df$Artikeln_Nr),sum))
colnames(df.art)=c('Article','nword')
df.art$JM=aggregate(df$nword*df$JM,list(df$Artikeln_Nr)
                    ,sum)[,2]/df.art$nword
df.art$SN=aggregate(df$nword*df$SN,list(df$Artikeln_Nr)
                    ,sum)[,2]/df.art$nword
df.art$DC=aggregate(df$nword*df$DC,list(df$Artikeln_Nr)
                    ,sum)[,2]/df.art$nword
df.art$value=aggregate(df$value,list(df$Artikeln_Nr)
                       ,mean)[,2]

# creating a  gold standard out of the three annotations
# df.art$ann=rowSums(df.art[,c('SN','DC','JM')])/3 mean version
df.art$ann=apply(df.art[,c('SN','DC','JM')],1,median)

# rescaling value to optimize visual comparison
mean.ann=mean(df.art$ann)
sd.ann=sd(df.art$ann)
df.art$value.rs=(df.art$value-mean(df.art$value))/
        sd(df.art$value)*sd.ann+mean.ann

# visual comparison of goldstandard and automatic indicator
plot(df.art$value.rs,col='blue')
lines(df.art$ann)
cor(df.art$ann,df.art$value.rs)

# correlation(goldstandard,automatic) and # of words
steps=df.art$nword[order(df.art$nword)]
corl=matrix(NA,nrow=length(steps),3)
for (i in 1:(length(steps)-2)){
        long.art=df.art$nword>=steps[i]
        corl[i]=cor(df.art$ann[long.art],df.art$value.rs[long.art])
        corl[i,2]=cor.test(df.art$ann[long.art],df.art$value.rs[long.art])$p.value
        corl[i,3]=sum(long.art)
        }
```

```{r pic correlation and number of words,echo=FALSE}
plot(corl[,1],xaxt='n'
     ,main='Correlation and # of words'
     ,xlab='Number of words'
     ,ylab='Correlation')
xind=1:(length(steps)-1)
ind=corl[,2]<0.05
points(xind[which(ind)],corl[which(ind),1],col='red')
axis(1, at=1:length(steps), labels=steps)
legend('topleft'
       ,pch=1
       ,col=c('black','red')
       ,legend=c('correlation coefficients','significance at 5% level')
       )
# corl1=corl

```

```{r plotting significant and high correlation,eval=FALSE,echo=FALSE}
# plotting the results
long.art=df.art$nword>=steps[30]

plot(df.art$value[long.art],df.art$ann[long.art])
lines(df.art$JM[long.art])
lines(df.art$DC[long.art],col='blue')
lines(df.art$SN[long.art],col='red')
abline(h=0)
abline(h=-1,lty=2)
abline(h=1,lty=2)



```

# Checking for the relevance of nearly neutral words

Most of the words in SentiWS only have a low absolute sentiment value. The values range from -1 to 1. Two values are by far the most frequent: 0.004 occuring `r sum(valueword$wert==0.004)`next to zero, both positive and negative, art the most frequent. Two values stick out:   

```{r histogram valueword,echo=FALSE}
hist(valueword$wert,main='Histogram of word values\nSentiWS',xlab='value')

```



```{r are there any nearly neutral words that can be dropped?,cache=FALSE,eval=FALSE,echo=FALSE}
# ignoring small positive and negative value-words in different combinations
ext=round(0.05*1185/2)

testrange=valueword$wert[order(valueword$wert)]
testrange=unique(testrange)
nullp=which(testrange==0.004)
testrange=testrange[(nullp-ext):(nullp+(ext-1))]
border=t(combn(testrange, 2))
border=data.frame(border[border[,1]<border[,2],])
colnames(border)=c('lower','upper')
border$char=paste(border$lower,border$upper,sep=':')
I=nrow(border)
corI=data.frame(matrix(NA,I,1))

for (i in 1:I){
        
        valueword=read.csv(paste(DirCode,'/valueword.csv',sep=''))
        big.ind=valueword$wert<border$lower[i]|border$upper[i]<valueword$wert
        valueword=valueword[big.ind,]
        
        auto.sent=lapply(as.character(df[,'Text']),sentiment,valueword)
        auto=t(sapply(auto.sent,function(x) sentproc(x)))
        df.art[,border$char[i]]=aggregate(auto[,'value'],list(df$Artikeln_Nr)
                                          ,mean)[,2]
        }

# results are stored in "tdf.csv", the columns indicate the range left out

# correlation when small articles are excluded
window=10:45
differences=data.frame(matrix(NA,nrow=length(window),ncol=3))
row.names(differences)=steps[window]
colnames(differences)=c('difference','max','min')
for (i in window){
        long.art=df.art$nword>=steps[i]
        
        test=apply(df.art[long.art,border$char],2,function(x) cor(x,df.art$ann[long.art]))
        differences[as.character(steps[i]),1]=max(test)-min(test)
        differences[as.character(steps[i]),2]=max(test)
        differences[as.character(steps[i]),3]=min(test)
        }

```

```{r time reference}
# identifying columns containing time reference of the three annotators
ind.future.cols=grep('Zeitbezug',colnames(df))
zdf=df[,ind.future.cols]
# preparing an dataframe that indicates where 'Zukunft' = Future appears
zdfind=zdf
zdfind=zdfind*0
zdfind[grep('Zukunft',zdf[,1]),1]=1
zdfind[grep('Zukunft',zdf[,2]),2]=1
zdfind[grep('Zukunft',zdf[,3]),3]=1
# which of the paragraphs are unanimuously identified
zdfind$ind=rowSums(zdfind[,1:3])
zz=is.na(zdfind$ind)==F
sum(zz)
# throwing all past and present related texts in one basket (text), same with future
# first preparing texts
library("tm")

prepare.para<-function(text){
        text=strsplit(text,' ')
        text=sapply(text,function(x) x)
        text=gsub("[[:punct:]]", "", text)
        text=gsub("\\n", " ", text)
        # eliminating all nouns (capitals at the beginning)
        text=text[-grep('[A-Z]',text)]
        nostopwords=!text%in%stopwords('German')
        text=text[nostopwords]  
        return(text)
        }
vg=paste(df$Text[zz==F],collapse=' ')
vg=prepare.para(vg)
zk=paste(df$Text[zz==T],collapse=' ')
zk=prepare.para(zk)
not=!zk%in%vg
uni.zk=zk[not]
vg=strsplit(vg,' ')
vg=sapply(vg,function(x) x)
vg=gsub("[[:punct:]]", "", vg)
vg=gsub("\\n", " ", vg)
# eliminating all nouns (capitals at the beginning)
vg=vg[-grep('[A-Z]',vg)]
nostopwords=!vg%in%stopwords('German')
vg=vg[nostopwords]


```


```{r tm package for categorizing time,echo=FALSE}
docs <- as.character(df$Text)
docs <- Corpus(VectorSource(docs), readerControl = list(language = "De"))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation,preserve_intra_word_dashes = T)
docs <- tm_map(docs, removeWords, stopwords("german"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs,stemDocument,language='german')
docs <- tm_map(docs,tolower)

docs.s=docs
tdm<-DocumentTermMatrix(docs,control=list(
        weighting=function(x) weightTfIdf(x,normalize=FALSE)
        ,minDocFreq=10
        ))
inspect(tdm[1:10,1:10])
tdm <- as.data.frame(inspect(tdm))



```
```{r training set and evaluation,echo=FALSE}
# getting paragraph numbers and splitting up into training and evaluation
# future related paragraphs
para.fut=which(zz==T)
para.fut.train=para.fut[1:46]
para.fut.eval=para.fut[47:length(para.fut)]
# past and present related paragraphs
para.oth=which(zz==F)
train.quota=length(para.fut.train)/length(para.fut)
train.oth.n=round(train.quota*length(para.oth),0)
para.oth.train=para.oth[1:train.oth.n]
para.oth.eval=para.oth[(train.oth.n+1):length(para.oth)]

# putting indices together, create a dependent variable (1 if future, 0 if not)
para.train=c(para.fut.train,para.oth.train)
para.train.ind=c(rep(1,length(para.fut.train)),rep(0,length(para.oth.train)))

para.eval=c(para.fut.eval,para.oth.eval)
para.eval.ind=c(rep(1,length(para.fut.eval)),rep(0,length(para.oth.eval)))

library("e1071")
data.train=tdm[para.train,]
empty=colSums(data.train)==0
data.train=data.train[,!empty]
data.train=data.frame(time=as.factor(para.train.ind),data.train)
clsf=naiveBayes(time~ .,data=data.train)
data.eval=tdm[para.eval,!empty]
pred=predict(clsf,data.eval)
table(predict(clsf,data.eval), ,com$unambiguous[1:300])
```


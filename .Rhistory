seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste(year,'-',issue,'-',title,'.txt',sep=''))
return()
}
View(register)
fgettext(register$link,year,issue,register$title)
fgettext(input=register$link,year,issue,register$title)
fgettext(input=register$link[1],year,issue,register$title[1])
register$link[1]
unlist(register$link[1])
as.character(register$link[1])
fgettext<-function(input,year,issue,title){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
mittelteil<-convert_html_to_text(mittelteil)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
if (length(index)!=0){
weitere_seiten=fnextpages(plainhtml,index)
seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste(year,'-',issue,'-',title,'.txt',sep=''))
return()
}
fgettext(input=register$link[1],year,issue,register$title[1])
fgettext<-function(input,year,issue,title){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
mittelteil<-convert_html_to_text(mittelteil)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
if (length(index)!=0){
weitere_seiten=fnextpages(plainhtml,index)
seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste(year,'-',issue,'-',title,'.txt',sep=''))
#   return()
}
fgettext(input=register$link[1],year,issue,register$title[1])
getwd()
mainfile="C:/Users/Dirk/zeit"
setwd(mainfile)
registry<-function(year,issue){
issue_formated=c(paste(rep(0,9),1:9,sep=''),paste(10:12,sep=''))
input <-paste('http://www.zeit.de/',year,'/',issue_formated[issue],'/index',sep='')
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste('http://www.zeit.de/',year,'/',issue_formated[issue],sep=''),plainhtml)
register_raw=unique(plainhtml[index])
register_raw=register_raw[-grep(input,register_raw)]
t1=register_raw[1]
plainhtml_index_split=unlist(strsplit(register_raw,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
titles=plainhtml_index_split[grep('title=',plainhtml_index_split)+1]
descriptions=matrix(sapply(links,function(x){
y=unlist(strsplit(x,'/'))
y[length(y)]
}))
register=data.frame(link=unique(links)
,title=titles
,description=descriptions
,year=year
,issue=issue)
}
registry(2014,1)
registry<-function(year,issue){
issue_formated=c(paste(rep(0,9),1:9,sep=''),paste(10:12,sep=''))
input <-paste('http://www.zeit.de/',year,'/',issue_formated[issue],'/index',sep='')
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste('http://www.zeit.de/',year,'/',issue_formated[issue],sep=''),plainhtml)
register_raw=unique(plainhtml[index])
register_raw=register_raw[-grep(input,register_raw)]
t1=register_raw[1]
plainhtml_index_split=unlist(strsplit(register_raw,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
titles=plainhtml_index_split[grep('title=',plainhtml_index_split)+1]
descriptions=matrix(sapply(links,function(x){
y=unlist(strsplit(x,'/'))
y[length(y)]
}))
register=data.frame(link=unique(links)
,title=titles
,description=descriptions
,year=year
,issue=issue)
return(register)
}
test=registry(2014,1)
View(test)
file.path()
fgettext<-function(input,year,issue,title,outfile){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
mittelteil<-convert_html_to_text(mittelteil)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
if (length(index)!=0){
weitere_seiten=fnextpages(plainhtml,index)
seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste(year,'-',issue,'-',title,'.txt',sep=''))
#   return()
}
mainfile="C:/Users/Dirk/zeit"
year=2014
issue=1
tfile=dir.create(file.path(paste(mainfile,'/',year,'-',issue)))
dir.create(file.path(paste(mainfile,'/',year,'-',issue)))
paste(mainfile,'/',year,'-',issue)
dir.create(file.path(paste(mainfile,'/',year,'-',issue,sep='')))
source('C:/Users/Dirk/zeit/Getting_texts.R', echo=TRUE)
main
mainfile
dir
dir()
tfile%in%dir()
dir()==tfile
dir()==paste(year,'-',issue)
paste(year,'-',issue)
dir()==paste(year,'-',issue,sep='')
(sum(dir()==paste(year,'-',issue,sep=''))==0)
source('~/.active-rstudio-document', echo=TRUE)
source('C:/Users/Dirk/zeit/Getting_texts.R', echo=TRUE)
i
title
source('~/.active-rstudio-document', echo=TRUE)
colnames(register)
source('C:/Users/Dirk/zeit/Getting_texts.R', echo=TRUE)
file
outfile
tfile
source('C:/Users/Dirk/zeit/Getting_texts.R', echo=TRUE)
write.csv(register,paste(tfile,'/register.csv'))
write.csv(register,paste(tfile,'/register.csv',sep=''))
rm(i,rm(register,issue,narticle,year))
rm(i,register,issue,narticle,year)
Dir<-DirSource(directory=tfile,recursive = TRUE)
Publikationen <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
x <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
zeitcorp <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
Dir<-DirSource(directory=tfile,recursive = TRUE)
zeitcorp <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
zeitcorp<-Corpus(txt,readerControl = list(reader = readPlain,language="de"))
zeitcorp <- tm_map(zeitcorp, stripWhitespace)
zeitcorp <- tm_map(zeitcorp, removeNumbers)
zeitcorp <- tm_map(zeitcorp, removePunctuation)
zeitcorp <- tm_map(zeitcorp, tolower)
zeitcorp <- tm_map(zeitcorp, function(x){removeWords(x,
c("dass",stopwords("german")))})
inspect(zeitcorp)
?searchFullText
Tdmzeit <- termDocumentMatrix(zeitcorp)
Tdmzeit <- TermDocumentMatrix(zeitcorp)
write.csv(zeitcorp[[1]],'test.csv')
write.csv(zeitcorp[[1]],'test.txt')
Dir<-DirSource(directory=tfile,recursive = TRUE)
zeitcorp <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
write.csv(zeitcorp[[1]],'test.txt')
write.csv(zeitcorp[[1]],'test.txt')
source('~/.active-rstudio-document', echo=TRUE)
register=registry(year,issue)
register=registry(2014,1)
colnames(register)
input(register[1,1])
input<-register[1,1]
input
plainhtml=readLines(input,encoding='UTF-8')
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
mittelteil=readHTML(mittelteil)
mittelteil=readHTML(mittelteil,mittelteil.html,mittelteil.txt)
mittelteil
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
mittelteil<-convert_html_to_text(mittelteil)
head(mittelteil)
plainhtml=readLines(input,encoding='UTF-8')
head(plainhtml)
plainhtml=readLines(input,encoding='UTF-8')
plainhtml
unlink(input)
head(palinhtml)
head(plainhtml)
head(plainhtml,50)
?write.csv
# Setting directory and settings -------------------------------------------------------
mainfile="C:/Users/Dirk/zeit"
year=2014
issue=1
setwd(mainfile)
# loading libraries -------------------------------------------------------
library(tm)
library(RCurl)
library(XML)
# Getting links -----------------------------------------------------------
registry<-function(year,issue){
issue_formated=c(paste(rep(0,9),1:9,sep=''),paste(10:12,sep=''))
input <-paste('http://www.zeit.de/',year,'/',issue_formated[issue],'/index',sep='')
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste('http://www.zeit.de/',year,'/',issue_formated[issue],sep=''),plainhtml)
register_raw=unique(plainhtml[index])
register_raw=register_raw[-grep(input,register_raw)]
t1=register_raw[1]
plainhtml_index_split=unlist(strsplit(register_raw,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
titles=plainhtml_index_split[grep('title=',plainhtml_index_split)+1]
descriptions=matrix(sapply(links,function(x){
y=unlist(strsplit(x,'/'))
y[length(y)]
}))
register=data.frame(link=unique(links)
,title=titles
,description=descriptions
,year=year
,issue=issue)
return(register)
}
# Getting texts -----------------------------------------------------------
fnextpages<-function(plainhtml,index){
plainhtml_index=plainhtml[index]
plainhtml_index_split=unlist(strsplit(plainhtml_index,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
unique(links)
}
convert_html_to_text <- function(html) {
# extracted from: convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
fgettext<-function(input,year,issue,title,outfile){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
#   mittelteil=readHTML(mittelteil,mittelteil.html,mittelteil.txt)
mittelteil<-convert_html_to_text(mittelteil)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
if (length(index)!=0){
weitere_seiten=fnextpages(plainhtml,index)
seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste(outfile,'/',title,'.txt',sep=''),fileEncoding='UTF-8')
#   return()
}
# running the code --------------------------------------------------------
register=registry(year,issue)
narticle=nrow(register)
tfile=paste(mainfile,'/',year,'-',issue,sep='')
if (sum(dir()==paste(year,'-',issue,sep=''))==0){
dir.create(file.path(tfile))
}
for (i in 1:1){
# for (i in 1:narticle){
fgettext(input=register$link[i],year=year,issue=issue,title=register$description[i],outfile=tfile)
}
write.csv(register,paste(tfile,'/register.csv',sep=''))
rm(i,register,issue,narticle,year)
# Setting directory and settings -------------------------------------------------------
mainfile="C:/Users/Dirk/zeit"
year=2014
issue=1
setwd(mainfile)
# loading libraries -------------------------------------------------------
library(tm)
library(RCurl)
library(XML)
# Getting links -----------------------------------------------------------
registry<-function(year,issue){
issue_formated=c(paste(rep(0,9),1:9,sep=''),paste(10:12,sep=''))
input <-paste('http://www.zeit.de/',year,'/',issue_formated[issue],'/index',sep='')
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste('http://www.zeit.de/',year,'/',issue_formated[issue],sep=''),plainhtml)
register_raw=unique(plainhtml[index])
register_raw=register_raw[-grep(input,register_raw)]
t1=register_raw[1]
plainhtml_index_split=unlist(strsplit(register_raw,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
titles=plainhtml_index_split[grep('title=',plainhtml_index_split)+1]
descriptions=matrix(sapply(links,function(x){
y=unlist(strsplit(x,'/'))
y[length(y)]
}))
register=data.frame(link=unique(links)
,title=titles
,description=descriptions
,year=year
,issue=issue)
return(register)
}
# Getting texts -----------------------------------------------------------
fnextpages<-function(plainhtml,index){
plainhtml_index=plainhtml[index]
plainhtml_index_split=unlist(strsplit(plainhtml_index,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
unique(links)
}
convert_html_to_text <- function(html) {
# extracted from: convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE)
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
fgettext<-function(input,year,issue,title,outfile){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
#   mittelteil=readHTML(mittelteil,mittelteil.html,mittelteil.txt)
mittelteil<-convert_html_to_text(mittelteil)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
if (length(index)!=0){
weitere_seiten=fnextpages(plainhtml,index)
seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste(outfile,'/',title,'.txt',sep=''),fileEncoding='UTF-8')
#   return()
}
# running the code --------------------------------------------------------
register=registry(year,issue)
narticle=nrow(register)
tfile=paste(mainfile,'/',year,'-',issue,sep='')
if (sum(dir()==paste(year,'-',issue,sep=''))==0){
dir.create(file.path(tfile))
}
for (i in 1:1){
# for (i in 1:narticle){
fgettext(input=register$link[i],year=year,issue=issue,title=register$description[i],outfile=tfile)
}
write.csv(register,paste(tfile,'/register.csv',sep=''))
rm(i,register,issue,narticle,year)
read.table(paste('C:/Users/Dirk/zeit/2014-1/tuerkei-erdogan-korruption.txt',fileEncoding='UTF-8')
)
read.table('C:/Users/Dirk/zeit/2014-1/tuerkei-erdogan-korruption.txt',fileEncoding='UTF-8')
test=read.table('C:/Users/Dirk/zeit/2014-1/tuerkei-erdogan-korruption.txt',fileEncoding='UTF-8')
test
?DirSource
Dir<-DirSource(directory=tfile,encoding='UTF-8',recursive = TRUE)
zeitcorp <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
zeitcorp[[1]]
zeitcorp$tuerkei-erdogan-korruption.txt
zeitcorp$'tuerkei-erdogan-korruption.txt'
?Corpus
?read.csv
test=read.csv('C:/Users/Dirk/zeit/2014-1/tuerkei-erdogan-korruption.txt',fileEncoding='UTF-8')
View(test)
test
?readLines
test=readLines('C:/Users/Dirk/zeit/2014-1/tuerkei-erdogan-korruption.txt')
test
test=readLines('C:/Users/Dirk/zeit/2014-1/tuerkei-erdogan-korruption.txt',encoding='UTF-8')
test
input=paste(tfile,tuerkei-erdogan-korruption.txt,sep='')
input=paste(tfile,'tuerkei-erdogan-korruption.txt',sep='')
plainhtml=readLines(input,encoding='UTF-8')
register=registry(year,issue)
year=2014
issue=1
register=registry(year,issue)
input=register[1,1]
input
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
plainhtml
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
head(plainhtml)
head(plainhtml,50)
head(mittleteil,50)
head(mittelteil,50)
mittelteil<-convert_html_to_text(mittelteil)
head(mittelteil,50)
?htmlParse
convert_html_to_text <- function(html) {
# extracted from: convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE,encoding='UTF-8')
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
#   mittelteil=readHTML(mittelteil,mittelteil.html,mittelteil.txt)
mittelteil<-convert_html_to_text(mittelteil)
head(mittelteil,50)
for (i in 1:1){
# for (i in 1:narticle){
fgettext(input=register$link[i],year=year,issue=issue,title=register$description[i],outfile=tfile)
}
Dir<-DirSource(directory=tfile,encoding='UTF-8',recursive = TRUE)
zeitcorp <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
zeitcorp$'tuerkei-erdogan-korruption.txt'
zeitcorp <- tm_map(zeitcorp, stripWhitespace)
zeitcorp <- tm_map(zeitcorp, removeNumbers)
zeitcorp <- tm_map(zeitcorp, removePunctuation)
zeitcorp <- tm_map(zeitcorp, tolower)
zeitcorp <- tm_map(zeitcorp, function(x){removeWords(x,
c("dass",stopwords("german")))})
zeitcorp$'tuerkei-erdogan-korruption.txt'
zeitcorp <- stemDocument(zeitcorp, language = meta(zeitcorp, "german"))
zeitcorp <- tm_map(zeitcorp,stemDocument,language = meta(zeitcorp, "german"))
install.packages("SnowballC")
library(SnowballC)
zeitcorp <- tm_map(zeitcorp,stemDocument,language = meta(zeitcorp, "german"))
zeitcorp <- tm_map(zeitcorp,function(x){stemDocument(x,language = meta(zeitcorp, "german"))})
test=zeitcorp$'tuerkei-erdogan-korruption.txt'
stemDocument(test,language=meta(test,'German'))
?stemDocument
stemDocument(test,language='German')
stemDocument(test,language='german')
zeitcorp <- tm_map(zeitcorp,function(x){stemDocument(x,language = "german")})

anti
rm(anti)
source('~/.active-rstudio-document', echo=TRUE)
anti
version
install.packages("astsa")
library("astsa", lib.loc="C:/Users/Dirk/Documents/R/win-library/3.0")
data(birth)
plot(birth)
lag2.plot(birth)
install.packages("tm")
library("tm", lib.loc="C:/Users/Dirk/Documents/R/win-library/3.0")
stopwords("german")
install.packages("qdap")
library("qdap", lib.loc="C:/Users/Dirk/Documents/R/win-library/3.0")
txt <- system.file("texts", "txt", package = "tm")
(ovid <- Corpus(DirSource(txt, encoding = "UTF-8"),
readerControl = list(language = "lat")))
txt
ovid
fix(ovid)
inspect(ovid)
scores.word_stats(ovid)
scores.word(ovid)
freq.terms(ovid)
require(qdap)
findFreqTerms(ovid)
reuters <- tm_map(reuters, stripWhitespace)
reuters <- tm_map(reuters, as.PlainTextDocument)
reut21578 <- system.file("texts", "crude", package = "tm")
reuters <- Corpus(DirSource(reut21578),
readerControl = list(reader = readReut21578XML))
reuters <- tm_map(reuters, as.PlainTextDocument)
findFreqTerms(reuters)
source('F:/Hertie School of Governance/Vorlesung/Hertie/homework time series 2.R', echo=TRUE)
rstudio::viewData(df)
library(forecast)
library(stats)
# getting the data --------------------------------------------------------
sFolder='f:/Hertie School of Governance/Vorlesung/Hertie'
sFile="/unemployment.csv"
# loading necessary packages ----------------------------------------------
install.packages('dyn')
install.packages('vars')
install.packages('outliers')
install.packages('forecast')
install.packages('tseries')
install.packages('lmtest')
library('dyn')
library('vars')
library('outliers')
library('forecast')
library('tseries')
library('lmtest')
install.packages("forecast")
install.packages("tseries")
install.packages("tseries")
sFolder='d:/Hertie School of Governance/Vorlesung/Hertie'
sFile="/income and consumption.csv"
df=read.csv(paste(sFolder,sFile,sep='')
, sep=";"
, dec="."
,row.names=1
)
sFolder='f:/Hertie School of Governance/Vorlesung/Hertie'
sFile="/income and consumption.csv"
df=read.csv(paste(sFolder,sFile,sep='')
, sep=";"
, dec="."
,row.names=1
)
rstudio::viewData(df)
rstudio::viewData(df)
df
rstudio::viewData(df)
library("rstudio", lib.loc="C:/Program Files/RStudio/R/library")
plot(df)
VAR(df,lag.max=3,ic='SC')
test=VAR(df,lag.max=3,ic='SC')
test
str(test)
test$varresult
coefficients(test)
coef(test)
coef(test)[,1]
apply(coef(test),1,function{x[,4]})
apply(coef(test),1,function(x)x[,4])
apply(coef(test),2,function(x)x[,4])
t1=coef(test)
t1=unlist(coef(test))
fix(`t1`)
sapply(coef(test),unlist)
apply(coef(test),2,function(x)x[,4])
lapply(coef(test),function(x)x[,4])
unlist(lapply(coef(test),function(x)x[,4]))
t1=unlist(lapply(coef(test),function(x)x[,4]))
t1
t1=matrix(unlist(lapply(coef(test),function(x)x[,4])),nrow=2)
rstudio::viewData(`t1`)
library()
rstudio::viewData(df)
rstudio::viewData(df)
rstudio::viewData(df)
rstudio::viewData(df)
rstudio::viewData(df)
rstudio::viewData(`t1`)
head(t1)
tetst
test
t1
test
t1=matrix(unlist(lapply(coef(test),function(x)x[,1])),nrow=2)
t1
coef(test)
coef(test)[1]
coef(test)[1][,1]
coef(test)[[1]][,1]
t2=coef(test)
lapply(coef(test),function(x)[,1])
lapply(coef(test),function(x)x[,1])
sapply(coef(test),function(x)x[,1])
t3=sapply(coef(test),function(x)x[,1])
test
t(t3)
t3=t(sapply(coef(test),function(x)x[,1]))
rstudio::viewData(`t3`)
t3
head(df1)
tail(df)
tail(df,2)
t(tail(df,2))
t4=t(tail(df,2))
rev(t4)
t5=apply(t4,2,rev)
t5
t6=apply(tail(df,2),2,rev)
t6
t6=t(apply(tail(df,2),2,rev))
t6
t3
matrix(t6,ncol=1)
t3[,2:ncol(t3)]*matrix(t6,ncol=1)
t3[,2:ncol(t3)]/*/matrix(t6,ncol=1)
t3[,2:ncol(t3)]%*%matrix(t6,ncol=1)
diag(2)
diag(4)
diag(2)
source('F:/Log vs level/logvslevel.R', echo=TRUE)
predict(test)
?predict
predict(test,2)
predict(test,n.ahead=2)
?predict
11509.35+57.66274
predict(test,n.ahead=2,ci=1)
predict(test,n.ahead=2,ci=0)
tt=predict(test,n.ahead=2,ci=0)
str(tt)
tt=predict(test,n.ahead=2,ci=0)
tt
tt=predict(test,n.ahead=20,ci=0.95)
tt
tt[,1]
tt$con[,1]
tt$con
tt[1]
tt[2]
tt[3]
tt[4]
tt[4]$fcst
tt$fcst
tt$fcst$con
library(HSAUR)
install.packages("HSAUR")
library(HSAUR)
library(HSAUR)
data("heptathlon", package = "HSAUR")
heptathlon$hurdles <- max(heptathlon$hurdles) - heptathlon$hurdles
heptathlon$run200m <- max(heptathlon$run200m) - heptathlon$run200m
heptathlon$run800m <- max(heptathlon$run800m) - heptathlon$run800m
round(cor(heptathlon), 2)
plot(heptathlon)
heptathlon_pca <- prcomp(heptathlon, scale = TRUE)
print(heptathlon_pca)
colSums((heptathlon_pca[2]$rotation))
summary(heptathlon_pca)
center <- heptathlon_pca$center
scale <- heptathlon_pca$scale
hm <- as.matrix(heptathlon)
install.packages("phtt")
library(phtt)
data(cigar)
data(Cigar)
rstudio::viewData(Cigar)
head(Cigar)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
dat[1:rec,]
rec
View(dat)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
save.image("F:/Log vs level/US_Ergebnisse.RData")
lrec
View(dat)
?Corpus
library("tm", lib.loc="~/R/win-library/3.0")
?Corpus
thepage = readLines(http://www.faz.net/')
thepage = readLines(http://www.faz.net')
thepage = readLines('http://www.faz.net')
thepage
thepage = readHTML('http://www.faz.net')
thepage = Corpus('http://www.faz.net',readerControl=list(language='German'))
source('~/.active-rstudio-document', echo=TRUE)
pagetree
x<- xpathSApply(pagetree, "//*/table", xmlValue)
x
head(x)
source('~/.active-rstudio-document', echo=TRUE)
head(x)
thepage = Corpus('http://www.faz.net',readerControl=list(reader=readHTML,language='German'))
library("tm", lib.loc="~/R/win-library/3.0")
thepage = Corpus('http://www.faz.net',readerControl=list(reader=readHTML,language='German'))
source('~/.active-rstudio-document', echo=TRUE)
Publikationen <- Corpus(x=http://www.faz.net',
readerControl = list(reader = readHTML,
language="de"))
Publikationen <- Corpus(x='http://www.faz.net',readerControl = list(reader = readHTML,language="de"))
Dir<-DirSource(directory="http://www.faz.net",recursive = TRUE)
?DirSource
wx='http://www.faz.net'
x=readLines(wx)
Publikationen <- Corpus(x,readerControl = list(reader = readHTML,language="de"))
getSources
Publikationen <- Corpus(VectorSource=x,readerControl = list(reader = readHTML,language="de"))
xs=VectorSource(x)
Publikationen <- Corpus(xs,readerControl = list(reader = readHTML,language="de"))
xs
?readLines
?Corpus
getReaders
Publikationen <- Corpus(xs,readerControl = list(reader = readXML,language="de"))
?Corpus
Publikationen <- Corpus(xs,readerControl = list(reader = readPlain,language="de"))
readHTML<-function(elem, language, id){
content <- system2("html2text", shQuote(elem$uri), stdout = TRUE)
PlainTextDocument(content, id = id, language = language,
origin=sub("/.*", "", sub(".*Publikationen/*", "", elem$uri)))
}
Publikationen <- Corpus(xs,readerControl = list(reader = readHTML,language="de"))
install.packages("tm.plugin.webmining")
library("tm.plugin.webmining", lib.loc="~/R/win-library/3.0")
x=extractContentDOM(wx)
x=extractHTMLStrip(wx)
?extractHTMLStrip
x=readLines(wx)
x=extractHTMLStrip(x)
library(tm.plugin.webmining)
wx='http://www.zeit.de/2014/01/tuerkei-erdogan-korruption'
x=readLines(wx)
library(tm.plugin.webmining)
version()
info()
sessionInfo()
swirl()
library('swirl')
swirl()
sapply(flags,unique)
vapply(flags,uniqe,numeric(1))
ok()
sapply(flags,class)
vapply(flags,class,character(1))
?tapply
table(flags$landmass)
table(flags$animate)
tapply(flags$animate,flags$landmass,mean)
tapply(flags$population,flags$red,summary)
flags$red
View(flags)
tapply(flags$population,flags$landmass,summary)
library(tm.plugin.webmining)
table(flags)
table(flags$Population)
table(flags$population)
library(tm.plugin.webmining)
x=extractHTMLStrip(x)
?extractHTMLStrip
x=extractHTMLStrip(wx)
x=readLines(wx)
head(x)
x=extractHTMLStrip(x)
x
?readLines
x=readLines(wx,encoding=UTF-8)
x=readLines(wx,encoding='UTF-8')
x
x=extractHTMLStrip(x)
xs=VectorSource(x)
library(tm)
xs=VectorSource(x)
Publikationen <- Corpus(xs,readerControl = list(reader = readHTML,language="de"))
Publikationen <- Corpus(xs,readerControl = list(reader = readPlain,language="de"))
inspect(Publikationen)
w = getURL("http://www.omegahat.org/RCurl/index.html")
w
htmlTreeParse(w)
library("XML", lib.loc="~/R/win-library/3.0")
htmlTreeParse(w)
?htmlTreeParse
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
search(htmlToText)
??htmlToText
source('~/.active-rstudio-document', echo=TRUE)
source.with.encoding('~/.active-rstudio-document', encoding='UTF-8', echo=TRUE)
x<-Corpus(txt,readerControl = list(reader = readPlain,language="de"))
library(tm)
x<-Corpus(txt,readerControl = list(reader = readPlain,language="de"))
txt <- htmlToText(input)
txt
input
x=readLines(wx,encoding='UTF-8')
wx='http://www.zeit.de/2014/01/tuerkei-erdogan-korruption'
x=readLines(wx,encoding='UTF-8')
x=extractHTMLStrip(x)
library(tm.plugin.webmining)
x=extractHTMLStrip(x)
x
txt <- htmlToText(input)
x=readLines(wx,encoding='UTF-8')
x
source('C:/Users/Dirk/zeit/Getting_texts.R', echo=TRUE)
head(Tdmzeit)
Mtdmzeit<- as.matrix(Tdmzeit)
View(Mtdmzeit)
head(Mtdmzeit)
?TermDocumentMatrix
zeitcorp$'michael-schumacher-schaedel-hirn-trauma.txt'
?sink
save(Tdmzeit, file = paste(tfile,'dtm-',year,'-',issue,".RData")
)
year=2014
issue=2
save(Tdmzeit, file = paste(tfile,'dtm-',year,'-',issue,".RData"))
save(Tdmzeit, file = paste(tfile,'dtm-',year,'-',issue,".RData",sep=''))
save(Tdmzeit, file = paste(tfile,'/dtm-',year,'-',issue,".RData",sep=''))
rohtext <- Corpus(x=Dir,
readerControl = list(reader = readPlain,
language="de"))
save(Tdmzeit,rohtext, file = paste(tfile,'/dtm-rawtext',year,'-',issue,".RData",sep=''))
save(Tdmzeit,rohtext, file = paste(tfile,'/dtm-rawtext-',year,'-',issue,".RData",sep=''))
View(Mtdmzeit)
install.packages("kernlab")
library(kernlab)
test.class<-DMetaData(Tdmzeit)
test.class<-DMetaData(zeitcorp)
kurz<-Mtdmzeit[which(rownames(Mtdmzeit)%in%findFreqTerms(Tdmzeit,10,Inf)),]
kurz<-Mtdmzeit[which(rownames(Mtdmzeit)%in%findFreqTerms(Tdmzeit,100,Inf)),]
View(kurz)
kurz<-Mtdmzeit[which(rownames(Mtdmzeit)%in%findFreqTerms(Tdmzeit,50,Inf)),]
View(kurz)
register=registry(year,issue)
View(register)
test=paste(register$title,collapse='-')
test
test=paste(register$title,collapse=' ')
test
?Corpus
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
getsources?
??
?getsources
test=DataframeSource(test)
test=DataframeSource(as.data.frame(test))
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
test=paste(register$title,collapse=' ')
test=VectorSource(as.data.frame(test))
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
Mtest=as.matrix(test)
dtmtest=DocumentTermMatrix(test)
Mtest=as.matrix(dtmtest)
View(Mtest)
test=paste(register$title,collapse=' ')
test=VectorSource(test)
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
dtmtest=DocumentTermMatrix(test)
Mtest=as.matrix(dtmtest)
View(Mtest)
test=VectorSource(register$title)
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
dtmtest=DocumentTermMatrix(test)
Mtest=as.matrix(dtmtest)
View(Mtest)
kurz<-Mtest[which(rownames(Mtest)%in%findFreqTerms(dtmtest,10,Inf)),]
kurz<-Mtest[which(rownames(Mtest)%in%findFreqTerms(dtmtest,3,Inf)),]
kurz<-Mtest[which(rownames(Mtest)%in%findFreqTerms(dtmtest,1,Inf)),]
findFreqTerms(dtmtest,4,Inf))
findFreqTerms(dtmtest,4,Inf)
dtmtest <- tm_map(dtmtest, tolower)
dtmtest <- tm_map(dtmtest, function(x){removeWords(x,
c("dass",stopwords("german")))})
dtmtest <- tm_map(dtmtest,function(x){stemDocument(x,language = "german")})
Mtest=as.matrix(dtmtest)
test=paste(register$title,collapse=' ')
test=VectorSource(register$title)
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
dtmtest=DocumentTermMatrix(test)
dtmtest <- tm_map(dtmtest, tolower)
test=paste(register$title,collapse=' ')
test=VectorSource(register$title)
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
dtmtest <- tm_map(dtmtest, tolower)
dtmtest <- tm_map(dtmtest, function(x){removeWords(x,
c("dass",stopwords("german")))})
dtmtest <- tm_map(dtmtest,function(x){stemDocument(x,language = "german")})
dtmtest=DocumentTermMatrix(test)
test=paste(register$title,collapse=' ')
test=VectorSource(register$title)
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
dtmtest <- tm_map(dtmtest, tolower)
test=paste(register$title,collapse=' ')
test=VectorSource(register$title)
test=Corpus(test, readerControl = list(reader = readPlain,
language="de"))
test <- tm_map(test, tolower)
test <- tm_map(test, function(x){removeWords(x,
c("dass",stopwords("german")))})
test <- tm_map(test,function(x){stemDocument(x,language = "german")})
dtmtest=DocumentTermMatrix(test)
Mtest=as.matrix(dtmtest)
findFreqTerms(dtmtest,4,Inf)
findFreqTerms(dtmtest,3,Inf)
findFreqTerms(dtmtest,2,Inf)
input<-http://www.zeit.de/2013/
input<-'http://www.zeit.de/2013'
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
index
write.csv(plainhtml,'test.txt')
paste(input,'/seite-',sep='')
index=grep(paste(input,'/index/seite-',sep=''),plainhtml)
index
plainhtml[index]
register$title='kinderarbeit-bolivien'
View(register)
year=2014
issue=1
register=registry(year,issue)
register$title=='kinderarbeit-bolivien'
View(register)
register$description=='kinderarbeit-bolivien'
register[22,]
narticle=nrow(register)
tfile=paste(mainfile,'/',year,'-',issue,sep='')
if (sum(dir()==paste(year,'-',issue,sep=''))==0){
dir.create(file.path(tfile))
}
fgettext(input=register$link[22],year=year,issue=issue,title=register$description[22],outfile=tfile)
input<-'http://www.zeit.de/2013'
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste(input,'/index/seite-',sep=''),plainhtml)
plainhtml[index]
input<-'http://www.zeit.de/2013/index/seite-2'
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste(input,'/index/seite-',sep=''),plainhtml)
plainhtml[index]
write.csv(plainhtml,'test.txt')
write.csv(plainhtml,'test.txt')
paste(input,'/index/seite-',sep='')
input<-'http://www.zeit.de/2013/index/seite-4'
plainhtml=readLines(input,encoding='UTF-8')
input
write.csv(plainhtml,'test.txt')
input<-'http://www.zeit.de/2013/index/seite-3'
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
write.csv(plainhtml,'test.txt')
input<-paste('http://www.zeit.de/',year,'/index/seite-3',sep='')
input
year=2013
input<-paste('http://www.zeit.de/',year,'/index/seite-3',sep='')
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
index=grep(paste('http://www.zeit.de/',year,sep=''),plainhtml)
index
plainhtml[index]
fnextpages<-function(plainhtml,index){
plainhtml_index=plainhtml[index]
plainhtml_index_split=unlist(strsplit(plainhtml_index,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
unique(links)
}
fnextpages(plainhtml,index)
index_page=fnextpages(plainhtml,index)
page=fnextpages(plainhtml,index)
index_page=page[grep('index',page)]
index_page

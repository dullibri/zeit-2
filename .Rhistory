titles
titles_index=regexpr('title=\"(.*)["]?',plainhtml_redux)
titles_raw=regmatches(plainhtml_redux,titles_index)
titles_exist=gsub('h4 class=\"title\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
titles_index=regexpr('title=\"(.*)?"',plainhtml_redux)
titles_raw=regmatches(plainhtml_redux,titles_index)
titles_exist=gsub('h4 class=\"title\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
titles_index=regexpr('title=\"(.*)?h3',plainhtml_redux)
titles_raw=regmatches(plainhtml_redux,titles_index)
titles_exist=gsub('h4 class=\"title\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
titles_index=regexpr('title=\"(.*)h3?',plainhtml_redux)
titles_raw=regmatches(plainhtml_redux,titles_index)
titles_exist=gsub('h4 class=\"title\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
plainhtml_redux
write.csv(plainhtml_all,'test3.txt')
titles_index=regexpr('<h4 class="title=\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml_redux,titles_index)
titles_exist=gsub('h4 class=\"title=\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles_index=regexpr('<h4 class="title=\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml_redux,titles_index)
titles_exist=gsub('h4 class=\"title=\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('h4 class=\"title=\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
titles_index
titles=matrix(NA,nrow=length(links),ncol=1)
titles_index=regexpr('<h4 class="title=\"(.*)</h4',plainhtml)
titles_index
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('h4 class=\"title\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
titles_exist
titles=matrix(NA,nrow=length(links),ncol=1)
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('<h4 class=\"title\">|</h4','',titles_raw)
titles_exist
View(as.matrix(links))
View(as.matrix(titles))
View(as.matrix(titles_exists))
View(as.matrix(titles_exist))
View(as.matrix(links))
titles_index
(1:length(titles_index))[titles_index!=-1]
links_index
intersect(links_index,titles_index)
?intersect
setdiff(links_index,titles_index)
titles_index[53]
(1:length(titles_index))[titles_index!=-1]
(1:length(titles_index))[links_index!=-1]
plainhtml[52]
grep('putin',links)
grep('islamischer',links)
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/',issue_formated,'(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links_raw
plainhtml[47]
plainhtml[48]
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/',issue_formated,'(.*)" tit',sep=''),plainhtml)
links_index
links_raw=regmatches(plainhtml,links_index)
links_raw
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/','(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links=gsub('" tit','',links_raw)
#         links_last=sapply(strsplit(links,'/'),function(x)x[6])
links
source('H:/git/zeit-2/Getting_topics.R', echo=TRUE)
year
issue_formated=as.character(issue)
if (issue<10){
issue_formated=paste(0,issue,sep='')
}
input <-paste('http://www.zeit.de/',year,'/',issue_formated,'/index',sep='')
if (url.exists(input)==F){return(NULL)}
plainhtml_all=readLines(input,encoding='UTF-8')
unlink(input)
# Getting ressorts and indeces, cutting off first part --------------------
ressort_index=grep('<li class="archiveressort">',plainhtml_all)
plainhtml=plainhtml_all[-c(1:(ressort_index[1]-1))]
ressort_start=ressort_index-(ressort_index[1]-1)
ressort_end=c(ressort_start[-1],length(plainhtml))-1
ressorts=plainhtml[ressort_start]
ressorts=gsub('<li class=\"archiveressort\">|</li>','',ressorts)
dfressorts=data.frame(ressorts,ressort_start,ressort_end,length=ressort_end-ressort_start+1)
mainressorts=paste('[Ww]irtschaft','[Pp]olitik','[Dd]ossier',sep='|')
fmainind <- function(dfressorts,mainressorts){
mdfressorts<-dfressorts[grep(mainressorts,dfressorts$ressorts),]
Nmressorts<-nrow(mdfressorts)
ind=matrix(NA,nrow=sum(mdfressorts[,'length']),ncol=1)
j=1
for (i in 1:Nmressorts){
ind[j:(j-1+mdfressorts[i,4])]=mdfressorts[i,2]:mdfressorts[i,3]
j=(j-1+mdfressorts[i,4])+1
}
return(ind)
}
ind=fmainind(dfressorts,mainressorts)
plainhtml=plainhtml[ind]
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/','(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links=gsub('" tit','',links_raw)
links
issue
titles=matrix(NA,nrow=length(links),ncol=1)
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('<h4 class=\"title\">|</h4','',titles_raw)
titles[titles_index!=-1]=titles_exist
titles
plainhtml_redux
issue_formated=as.character(issue)
if (issue<10){
issue_formated=paste(0,issue,sep='')
}
input <-paste('http://www.zeit.de/',year,'/',issue_formated,'/index',sep='')
if (url.exists(input)==F){return(NULL)}
plainhtml_all=readLines(input,encoding='UTF-8')
unlink(input)
# Getting ressorts and indeces, cutting off first part --------------------
ressort_index=grep('<li class="archiveressort">',plainhtml_all)
plainhtml=plainhtml_all[-c(1:(ressort_index[1]-1))]
ressort_start=ressort_index-(ressort_index[1]-1)
ressort_end=c(ressort_start[-1],length(plainhtml))-1
ressorts=plainhtml[ressort_start]
ressorts=gsub('<li class=\"archiveressort\">|</li>','',ressorts)
dfressorts=data.frame(ressorts,ressort_start,ressort_end,length=ressort_end-ressort_start+1)
mainressorts=paste('[Ww]irtschaft','[Pp]olitik','[Dd]ossier',sep='|')
fmainind <- function(dfressorts,mainressorts){
mdfressorts<-dfressorts[grep(mainressorts,dfressorts$ressorts),]
Nmressorts<-nrow(mdfressorts)
ind=matrix(NA,nrow=sum(mdfressorts[,'length']),ncol=1)
j=1
for (i in 1:Nmressorts){
ind[j:(j-1+mdfressorts[i,4])]=mdfressorts[i,2]:mdfressorts[i,3]
j=(j-1+mdfressorts[i,4])+1
}
return(ind)
}
ind=fmainind(dfressorts,mainressorts)
plainhtml=plainhtml[ind]
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/','(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links=gsub('" tit','',links_raw)
#         links_last=sapply(strsplit(links,'/'),function(x)x[6])
#         links_int=as.integer(links_last)
#         links=links[is.na(links_int)==T]
#         links=links[-grep('.xml',links)]
if (length(links)==0){return(NULL)}
plainhtml_redux=plainhtml[links_index!=-1]
plainhtml_redux
plainhtml
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_index
titles_raw=regmatches(plainhtml,titles_index)
titles_raw
titles_exist=gsub('<h4 class=\"title\">|</h4','',titles_raw)
titles_exist
source('~/.active-rstudio-document', echo=TRUE)
year
head(register)
issue
(length(links)!=nrow(titles))
# makes a register of an issue in a year.
# for each article it returns the weblink, its title, description, year,
# and issue
issue_formated=as.character(issue)
if (issue<10){
issue_formated=paste(0,issue,sep='')
}
input <-paste('http://www.zeit.de/',year,'/',issue_formated,'/index',sep='')
if (url.exists(input)==F){return(NULL)}
plainhtml_all=readLines(input,encoding='UTF-8')
unlink(input)
# Getting ressorts and indeces, cutting off first part --------------------
ressort_index=grep('<li class="archiveressort">',plainhtml_all)
plainhtml=plainhtml_all[-c(1:(ressort_index[1]-1))]
ressort_start=ressort_index-(ressort_index[1]-1)
ressort_end=c(ressort_start[-1],length(plainhtml))-1
ressorts=plainhtml[ressort_start]
ressorts=gsub('<li class=\"archiveressort\">|</li>','',ressorts)
dfressorts=data.frame(ressorts,ressort_start,ressort_end,length=ressort_end-ressort_start+1)
mainressorts=paste('[Ww]irtschaft','[Pp]olitik','[Dd]ossier',sep='|')
fmainind <- function(dfressorts,mainressorts){
mdfressorts<-dfressorts[grep(mainressorts,dfressorts$ressorts),]
Nmressorts<-nrow(mdfressorts)
ind=matrix(NA,nrow=sum(mdfressorts[,'length']),ncol=1)
j=1
for (i in 1:Nmressorts){
ind[j:(j-1+mdfressorts[i,4])]=mdfressorts[i,2]:mdfressorts[i,3]
j=(j-1+mdfressorts[i,4])+1
}
return(ind)
}
ind=fmainind(dfressorts,mainressorts)
plainhtml=plainhtml[ind]
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/','(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links=gsub('" tit','',links_raw)
#         links_last=sapply(strsplit(links,'/'),function(x)x[6])
#         links_int=as.integer(links_last)
#         links=links[is.na(links_int)==T]
#         links=links[-grep('.xml',links)]
if (length(links)==0){return(NULL)}
plainhtml_redux=plainhtml[links_index!=-1]
titles=matrix(NA,nrow=length(links),ncol=1)
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('<h4 class=\"title\">|</h4','',titles_raw)
titles=titles_exist
if (length(links)!=nrow(titles))
{}
(length(links)!=nrow(titles))
length(links
)
nrow(titles)
(length(links)!=length(titles))
registry<-function(year,issue){
# makes a register of an issue in a year.
# for each article it returns the weblink, its title, description, year,
# and issue
issue_formated=as.character(issue)
if (issue<10){
issue_formated=paste(0,issue,sep='')
}
input <-paste('http://www.zeit.de/',year,'/',issue_formated,'/index',sep='')
if (url.exists(input)==F){return(NULL)}
plainhtml_all=readLines(input,encoding='UTF-8')
unlink(input)
# Getting ressorts and indeces, cutting off first part --------------------
ressort_index=grep('<li class="archiveressort">',plainhtml_all)
plainhtml=plainhtml_all[-c(1:(ressort_index[1]-1))]
ressort_start=ressort_index-(ressort_index[1]-1)
ressort_end=c(ressort_start[-1],length(plainhtml))-1
ressorts=plainhtml[ressort_start]
ressorts=gsub('<li class=\"archiveressort\">|</li>','',ressorts)
dfressorts=data.frame(ressorts,ressort_start,ressort_end,length=ressort_end-ressort_start+1)
mainressorts=paste('[Ww]irtschaft','[Pp]olitik','[Dd]ossier',sep='|')
fmainind <- function(dfressorts,mainressorts){
mdfressorts<-dfressorts[grep(mainressorts,dfressorts$ressorts),]
Nmressorts<-nrow(mdfressorts)
ind=matrix(NA,nrow=sum(mdfressorts[,'length']),ncol=1)
j=1
for (i in 1:Nmressorts){
ind[j:(j-1+mdfressorts[i,4])]=mdfressorts[i,2]:mdfressorts[i,3]
j=(j-1+mdfressorts[i,4])+1
}
return(ind)
}
ind=fmainind(dfressorts,mainressorts)
plainhtml=plainhtml[ind]
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/','(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links=gsub('" tit','',links_raw)
#         links_last=sapply(strsplit(links,'/'),function(x)x[6])
#         links_int=as.integer(links_last)
#         links=links[is.na(links_int)==T]
#         links=links[-grep('.xml',links)]
if (length(links)==0){return(NULL)}
plainhtml_redux=plainhtml[links_index!=-1]
titles=matrix(NA,nrow=length(links),ncol=1)
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('<h4 class=\"title\">|</h4','',titles_raw)
titles=titles_exist
if (length(links)!=length(titles)){
register=data.frame(link=links
,title=NA
,year=year
,issue=issue)
}else{
register=data.frame(link=links
,title=titles
,year=year
,issue=issue)
}
return(register)
}
for (issue in 6:Nissue){
if (!'register'%in%ls()){
register=registry(year,issue)
}
register=rbind(register
,registry(year,issue))
}
year
tail(register)
for (year in 2005:2014){
# year=2014
input<-paste('http://www.zeit.de/',year,'/index/seite-3',sep='')
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
fnissue<-function(year,webpage){
# looks up the number of issues of one 'year' (mostly taking the value of 52 or 53)
# 'webpage' is the third page of the index page of the respective year (plaintext)
all_issue=matrix(NA,nrow=10,ncol=2)
for (i in 45:54){
index=grep(paste('http://www.zeit.de/',year,'/',i,'/index',sep=''),webpage)
all_issue[i-45+1,]=c(i,length(webpage[index]))
}
min(all_issue[all_issue[,2]==0,1])-1
}
Nissue=fnissue(year,plainhtml)
registry<-function(year,issue){
# makes a register of an issue in a year.
# for each article it returns the weblink, its title, description, year,
# and issue
issue_formated=as.character(issue)
if (issue<10){
issue_formated=paste(0,issue,sep='')
}
input <-paste('http://www.zeit.de/',year,'/',issue_formated,'/index',sep='')
if (url.exists(input)==F){return(NULL)}
plainhtml_all=readLines(input,encoding='UTF-8')
unlink(input)
# Getting ressorts and indeces, cutting off first part --------------------
ressort_index=grep('<li class="archiveressort">',plainhtml_all)
plainhtml=plainhtml_all[-c(1:(ressort_index[1]-1))]
ressort_start=ressort_index-(ressort_index[1]-1)
ressort_end=c(ressort_start[-1],length(plainhtml))-1
ressorts=plainhtml[ressort_start]
ressorts=gsub('<li class=\"archiveressort\">|</li>','',ressorts)
dfressorts=data.frame(ressorts,ressort_start,ressort_end,length=ressort_end-ressort_start+1)
mainressorts=paste('[Ww]irtschaft','[Pp]olitik','[Dd]ossier',sep='|')
fmainind <- function(dfressorts,mainressorts){
mdfressorts<-dfressorts[grep(mainressorts,dfressorts$ressorts),]
Nmressorts<-nrow(mdfressorts)
ind=matrix(NA,nrow=sum(mdfressorts[,'length']),ncol=1)
j=1
for (i in 1:Nmressorts){
ind[j:(j-1+mdfressorts[i,4])]=mdfressorts[i,2]:mdfressorts[i,3]
j=(j-1+mdfressorts[i,4])+1
}
return(ind)
}
ind=fmainind(dfressorts,mainressorts)
plainhtml=plainhtml[ind]
links_index=regexpr(paste('http://www.zeit.de/','(.*)','/','(.*)" tit',sep=''),plainhtml)
links_raw=regmatches(plainhtml,links_index)
links=gsub('" tit','',links_raw)
#         links_last=sapply(strsplit(links,'/'),function(x)x[6])
#         links_int=as.integer(links_last)
#         links=links[is.na(links_int)==T]
#         links=links[-grep('.xml',links)]
if (length(links)==0){return(NULL)}
plainhtml_redux=plainhtml[links_index!=-1]
titles=matrix(NA,nrow=length(links),ncol=1)
titles_index=regexpr('<h4 class="title\"(.*)</h4',plainhtml)
titles_raw=regmatches(plainhtml,titles_index)
titles_exist=gsub('<h4 class=\"title\">|</h4','',titles_raw)
titles=titles_exist
if (length(links)!=length(titles)){
register=data.frame(link=links
,title=NA
,year=year
,issue=issue)
}else{
register=data.frame(link=links
,title=titles
,year=year
,issue=issue)
}
return(register)
}
for (issue in 1:Nissue){
if (!'register'%in%ls()){
register=registry(year,issue)
}
register=rbind(register
,registry(year,issue))
}
}
save.image("H:/git/zeit-2/register.RData")
test=ls()
test
test[-16]
rm(list=test[-16])
rm(test)
save.image("H:/git/zeit-2/register.RData")
head(register)
sum(is.na(register$title)==T)
test=interaction(register$year,register$issue)
head(test)
test1=tapply(register,test)
test1=tapply(register$title,test,nrow)
head(test1)
test1=tapply(register$title,test,length)
head(test1)
View(as.matrix(test1))
hist(test1)
?dates
testdate=character(nrow(register))
for (i in 1:nrow(register)){
testdate[i]=paste(register$year,register$issue,' ')
}
i
fgettext<-function(input,number){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
mittelteil=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
#   mittelteil=readHTML(mittelteil,mittelteil.html,mittelteil.txt)
mittelteil<-convert_html_to_text(mittelteil)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
if (length(index)!=0){
weitere_seiten=fnextpages(plainhtml,index)
seiten_zahl=length(weitere_seiten)+1
for (i in 2:seiten_zahl){
plainhtml=readLines(weitere_seiten[i-1],encoding='UTF-8')
unlink(input)
zusaetzlicher_text=plainhtml[(grep('articleheader',plainhtml)+1):(grep('articlefooter',plainhtml)-1)]
zusaetzlicher_text<-convert_html_to_text(zusaetzlicher_text)
mittelteil=c(mittelteil,zusaetzlicher_text)
}
}
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=mittelteil[mittelteil!="\n"]
mittelteil=gsub('\n','',mittelteil,)
mittelteil=paste(mittelteil,collapse='')
write.csv(mittelteil,paste('H:/git/zeit-2/',number,'.txt',sep=''),fileEncoding='UTF-8')
#   return()
}
# getting the texts --------------------------------------------------------
for (i in 1:nrow(register)){
fgettext(input=register$link[i],year=register$year[i],issue=register$issue[i],title=register$title[i],outfile=tfile)
}
rm(i)
for (i in 1:nrow(register)){
fgettext(input=register$link[i],number=i)
}
rm(i)
fnextpages<-function(plainhtml,index){
plainhtml_index=plainhtml[index]
plainhtml_index_split=unlist(strsplit(plainhtml_index,'"'))
links=plainhtml_index_split[grep('http',plainhtml_index_split)]
unique(links)
}
convert_html_to_text <- function(html) {
# extracted from: convert_html_to_text <- function(html) {
doc <- htmlParse(html, asText = TRUE,encoding='UTF-8')
text <- xpathSApply(doc, "//text()[not(ancestor::script)][not(ancestor::style)][not(ancestor::noscript)][not(ancestor::form)]", xmlValue)
return(text)
}
# loading libraries -------------------------------------------------------
library(SnowballC)
library(tm)
library(RCurl)
library(XML)
library(kernlab)
# Getting links --
for (i in 1:nrow(register)){
fgettext(input=register$link[i],number=i)
}
rm(i)
i
input=register$link[16323]
input
input=register$link[16324]
input
class(input)
levels(input)
unlink(input)
fgettext<-function(input,number){
# input is the first http address of the text to be downloaded.
# input <-'http://www.zeit.de/2014/01/kinderarbeit-bolivien'# 3 seiten
input=as.character(input)
plainhtml=readLines(input,encoding='UTF-8')
unlink(input)
rm(input)
write.csv(plainhtml,paste('H:/git/zeit-2/',number,'.txt',sep=''),fileEncoding='UTF-8')
#   return()
}
for (i in 16324:nrow(register)){
fgettext(input=register$link[i],number=i)
}
i
input=register$link[5]
input
rm(input)
input
plainhtml=readLines(input,encoding='UTF-8')
input=register$link[5]
class(input)
levels(input)
input=as.character(input)
input
index=1
plainhtml=readLines(input,encoding='UTF-8')
class(input)
index=grep(paste(input,'/seite-',sep=''),plainhtml)
index
plainhtml[150]
plainhtml[423]
write.csv(plainhtml,'test.txt')
weitere_seiten=fnextpages(plainhtml,index)
weitere_seiten
